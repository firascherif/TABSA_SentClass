{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16099,"status":"ok","timestamp":1652645238660,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"zBPFB9bu2haM","outputId":"03101a5c-353c-405f-bb55-e5e1635a6cfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ktyxvV4XeDjG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652645238662,"user_tz":240,"elapsed":15,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"}},"outputId":"e3913d2a-578a-48b3-ebb9-e532bb17a947"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_notebooks/BERTSentclass\n"]}],"source":["cd drive/MyDrive/Colab_notebooks/BERTSentclass"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"znO7aUwk6yYX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652631010698,"user_tz":240,"elapsed":7279,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"}},"outputId":"48dc1dfe-ebfb-413d-aa82-df1166a5c9b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 27.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 64.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 46.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-17rYxwgpOU","outputId":"0b1f90f9-a818-49c5-dd04-e7e277f64640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 226k/226k [00:00<00:00, 269kB/s]\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 26.5kB/s]\n","Downloading: 100% 570/570 [00:00<00:00, 560kB/s]\n","tokenizer vocab len:  30524\n","Downloading: 100% 420M/420M [00:05<00:00, 74.9MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","05/15/2022 16:11:37 - INFO - __main__ -   Epoch = 1, Batch = 50, Batch loss = 0.586649, Avg loss (per batch) = 0.795082\n","05/15/2022 16:12:12 - INFO - __main__ -   Epoch = 1, Batch = 100, Batch loss = 0.743285, Avg loss (per batch) = 0.749498\n","05/15/2022 16:12:48 - INFO - __main__ -   Epoch = 1, Batch = 150, Batch loss = 0.540992, Avg loss (per batch) = 0.715464\n","05/15/2022 16:13:27 - INFO - __main__ -   Epoch = 1, Batch = 200, Batch loss = 0.543985, Avg loss (per batch) = 0.693411\n","05/15/2022 16:14:06 - INFO - __main__ -   Epoch = 1, Batch = 250, Batch loss = 0.374677, Avg loss (per batch) = 0.680020\n","05/15/2022 16:14:45 - INFO - __main__ -   Epoch = 1, Batch = 300, Batch loss = 0.464649, Avg loss (per batch) = 0.665072\n","05/15/2022 16:15:24 - INFO - __main__ -   Epoch = 1, Batch = 350, Batch loss = 0.481780, Avg loss (per batch) = 0.649535\n","05/15/2022 16:16:03 - INFO - __main__ -   Epoch = 1, Batch = 400, Batch loss = 0.415938, Avg loss (per batch) = 0.639643\n","05/15/2022 16:16:43 - INFO - __main__ -   Epoch = 1, Batch = 450, Batch loss = 0.452273, Avg loss (per batch) = 0.630953\n","05/15/2022 16:17:22 - INFO - __main__ -   Epoch = 1, Batch = 500, Batch loss = 0.249919, Avg loss (per batch) = 0.611690\n","05/15/2022 16:18:01 - INFO - __main__ -   Epoch = 1, Batch = 550, Batch loss = 0.207671, Avg loss (per batch) = 0.590400\n","05/15/2022 16:18:40 - INFO - __main__ -   Epoch = 1, Batch = 600, Batch loss = 0.072227, Avg loss (per batch) = 0.579246\n","05/15/2022 16:19:19 - INFO - __main__ -   Epoch = 1, Batch = 650, Batch loss = 0.091453, Avg loss (per batch) = 0.571431\n","05/15/2022 16:19:58 - INFO - __main__ -   Epoch = 1, Batch = 700, Batch loss = 0.011750, Avg loss (per batch) = 0.566720\n","05/15/2022 16:20:37 - INFO - __main__ -   Epoch = 1, Batch = 750, Batch loss = 0.141110, Avg loss (per batch) = 0.556275\n","05/15/2022 16:21:16 - INFO - __main__ -   Epoch = 1, Batch = 800, Batch loss = 0.769387, Avg loss (per batch) = 0.549933\n","05/15/2022 16:21:55 - INFO - __main__ -   Epoch = 1, Batch = 850, Batch loss = 0.422229, Avg loss (per batch) = 0.543431\n","05/15/2022 16:22:34 - INFO - __main__ -   Epoch = 1, Batch = 900, Batch loss = 0.033609, Avg loss (per batch) = 0.530331\n","05/15/2022 16:23:13 - INFO - __main__ -   Epoch = 1, Batch = 950, Batch loss = 0.240473, Avg loss (per batch) = 0.523940\n","05/15/2022 16:23:52 - INFO - __main__ -   Epoch = 1, Batch = 1000, Batch loss = 0.354658, Avg loss (per batch) = 0.511764\n","05/15/2022 16:24:30 - INFO - __main__ -   Epoch = 1, Batch = 1050, Batch loss = 0.025872, Avg loss (per batch) = 0.497560\n","05/15/2022 16:25:09 - INFO - __main__ -   Epoch = 1, Batch = 1100, Batch loss = 0.000473, Avg loss (per batch) = 0.474981\n","05/15/2022 16:25:48 - INFO - __main__ -   Epoch = 1, Batch = 1150, Batch loss = 0.000389, Avg loss (per batch) = 0.454349\n","05/15/2022 16:26:27 - INFO - __main__ -   Epoch = 1, Batch = 1200, Batch loss = 0.000288, Avg loss (per batch) = 0.435433\n","05/15/2022 16:27:06 - INFO - __main__ -   Epoch = 1, Batch = 1250, Batch loss = 0.000279, Avg loss (per batch) = 0.418027\n","05/15/2022 16:27:45 - INFO - __main__ -   Epoch = 1, Batch = 1300, Batch loss = 0.000217, Avg loss (per batch) = 0.401960\n","05/15/2022 16:28:24 - INFO - __main__ -   Epoch = 1, Batch = 1350, Batch loss = 0.000202, Avg loss (per batch) = 0.387081\n","05/15/2022 16:29:03 - INFO - __main__ -   Epoch = 1, Batch = 1400, Batch loss = 0.000185, Avg loss (per batch) = 0.373264\n","05/15/2022 16:29:19 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 16:33:56 - INFO - __main__ -   After 1.000000 epoch, Training loss = 0.367750, Training accuracy = 0.856125\n","05/15/2022 16:34:35 - INFO - __main__ -   Epoch = 2, Batch = 50, Batch loss = 1.115996, Avg loss (per batch) = 0.897239\n","05/15/2022 16:35:14 - INFO - __main__ -   Epoch = 2, Batch = 100, Batch loss = 0.093617, Avg loss (per batch) = 0.623723\n","05/15/2022 16:35:53 - INFO - __main__ -   Epoch = 2, Batch = 150, Batch loss = 0.007841, Avg loss (per batch) = 0.514474\n","05/15/2022 16:36:32 - INFO - __main__ -   Epoch = 2, Batch = 200, Batch loss = 0.043286, Avg loss (per batch) = 0.460479\n","05/15/2022 16:37:11 - INFO - __main__ -   Epoch = 2, Batch = 250, Batch loss = 0.173245, Avg loss (per batch) = 0.450051\n","05/15/2022 16:37:50 - INFO - __main__ -   Epoch = 2, Batch = 300, Batch loss = 0.012699, Avg loss (per batch) = 0.423643\n","05/15/2022 16:38:29 - INFO - __main__ -   Epoch = 2, Batch = 350, Batch loss = 0.040584, Avg loss (per batch) = 0.404562\n","05/15/2022 16:39:08 - INFO - __main__ -   Epoch = 2, Batch = 400, Batch loss = 0.015628, Avg loss (per batch) = 0.394879\n","05/15/2022 16:39:47 - INFO - __main__ -   Epoch = 2, Batch = 450, Batch loss = 0.011788, Avg loss (per batch) = 0.389798\n","05/15/2022 16:40:26 - INFO - __main__ -   Epoch = 2, Batch = 500, Batch loss = 0.695088, Avg loss (per batch) = 0.380010\n","05/15/2022 16:41:05 - INFO - __main__ -   Epoch = 2, Batch = 550, Batch loss = 0.002454, Avg loss (per batch) = 0.365898\n","05/15/2022 16:41:44 - INFO - __main__ -   Epoch = 2, Batch = 600, Batch loss = 0.009441, Avg loss (per batch) = 0.359278\n","05/15/2022 16:42:23 - INFO - __main__ -   Epoch = 2, Batch = 650, Batch loss = 0.075796, Avg loss (per batch) = 0.356460\n","05/15/2022 16:43:02 - INFO - __main__ -   Epoch = 2, Batch = 700, Batch loss = 0.001666, Avg loss (per batch) = 0.358873\n","05/15/2022 16:43:41 - INFO - __main__ -   Epoch = 2, Batch = 750, Batch loss = 0.022068, Avg loss (per batch) = 0.351036\n","05/15/2022 16:44:20 - INFO - __main__ -   Epoch = 2, Batch = 800, Batch loss = 1.657712, Avg loss (per batch) = 0.351962\n","05/15/2022 16:44:59 - INFO - __main__ -   Epoch = 2, Batch = 850, Batch loss = 0.672022, Avg loss (per batch) = 0.349213\n","05/15/2022 16:45:38 - INFO - __main__ -   Epoch = 2, Batch = 900, Batch loss = 0.001416, Avg loss (per batch) = 0.342195\n","05/15/2022 16:46:17 - INFO - __main__ -   Epoch = 2, Batch = 950, Batch loss = 0.062325, Avg loss (per batch) = 0.338919\n","05/15/2022 16:46:56 - INFO - __main__ -   Epoch = 2, Batch = 1000, Batch loss = 0.705352, Avg loss (per batch) = 0.332739\n","05/15/2022 16:47:35 - INFO - __main__ -   Epoch = 2, Batch = 1050, Batch loss = 0.389188, Avg loss (per batch) = 0.322928\n","05/15/2022 16:48:14 - INFO - __main__ -   Epoch = 2, Batch = 1100, Batch loss = 0.000320, Avg loss (per batch) = 0.308415\n","05/15/2022 16:48:53 - INFO - __main__ -   Epoch = 2, Batch = 1150, Batch loss = 0.000213, Avg loss (per batch) = 0.295017\n","05/15/2022 16:49:32 - INFO - __main__ -   Epoch = 2, Batch = 1200, Batch loss = 0.000161, Avg loss (per batch) = 0.282733\n","05/15/2022 16:50:11 - INFO - __main__ -   Epoch = 2, Batch = 1250, Batch loss = 0.000163, Avg loss (per batch) = 0.271431\n","05/15/2022 16:50:50 - INFO - __main__ -   Epoch = 2, Batch = 1300, Batch loss = 0.000148, Avg loss (per batch) = 0.260997\n","05/15/2022 16:51:29 - INFO - __main__ -   Epoch = 2, Batch = 1350, Batch loss = 0.000157, Avg loss (per batch) = 0.251336\n","05/15/2022 16:52:08 - INFO - __main__ -   Epoch = 2, Batch = 1400, Batch loss = 0.000130, Avg loss (per batch) = 0.242365\n","05/15/2022 16:52:24 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 16:56:59 - INFO - __main__ -   After 2.000000 epoch, Training loss = 0.238785, Training accuracy = 0.927578\n","05/15/2022 16:57:38 - INFO - __main__ -   Epoch = 3, Batch = 50, Batch loss = 0.324375, Avg loss (per batch) = 0.795484\n","05/15/2022 16:58:17 - INFO - __main__ -   Epoch = 3, Batch = 100, Batch loss = 0.002184, Avg loss (per batch) = 0.498974\n","05/15/2022 16:58:56 - INFO - __main__ -   Epoch = 3, Batch = 150, Batch loss = 0.002011, Avg loss (per batch) = 0.389130\n","05/15/2022 16:59:35 - INFO - __main__ -   Epoch = 3, Batch = 200, Batch loss = 0.002632, Avg loss (per batch) = 0.321607\n","05/15/2022 17:00:14 - INFO - __main__ -   Epoch = 3, Batch = 250, Batch loss = 0.350084, Avg loss (per batch) = 0.331079\n","05/15/2022 17:00:53 - INFO - __main__ -   Epoch = 3, Batch = 300, Batch loss = 0.003223, Avg loss (per batch) = 0.301597\n","05/15/2022 17:01:32 - INFO - __main__ -   Epoch = 3, Batch = 350, Batch loss = 0.001575, Avg loss (per batch) = 0.277512\n","05/15/2022 17:02:11 - INFO - __main__ -   Epoch = 3, Batch = 400, Batch loss = 0.001582, Avg loss (per batch) = 0.267481\n","05/15/2022 17:02:50 - INFO - __main__ -   Epoch = 3, Batch = 450, Batch loss = 0.009802, Avg loss (per batch) = 0.265549\n","05/15/2022 17:03:29 - INFO - __main__ -   Epoch = 3, Batch = 500, Batch loss = 0.801303, Avg loss (per batch) = 0.255474\n","05/15/2022 17:04:08 - INFO - __main__ -   Epoch = 3, Batch = 550, Batch loss = 0.001211, Avg loss (per batch) = 0.248014\n","05/15/2022 17:04:47 - INFO - __main__ -   Epoch = 3, Batch = 600, Batch loss = 0.000909, Avg loss (per batch) = 0.245540\n","05/15/2022 17:05:26 - INFO - __main__ -   Epoch = 3, Batch = 650, Batch loss = 0.001436, Avg loss (per batch) = 0.240980\n","05/15/2022 17:06:05 - INFO - __main__ -   Epoch = 3, Batch = 700, Batch loss = 0.000945, Avg loss (per batch) = 0.248698\n","05/15/2022 17:06:44 - INFO - __main__ -   Epoch = 3, Batch = 750, Batch loss = 0.548395, Avg loss (per batch) = 0.241882\n","05/15/2022 17:07:23 - INFO - __main__ -   Epoch = 3, Batch = 800, Batch loss = 0.444552, Avg loss (per batch) = 0.242604\n","05/15/2022 17:08:02 - INFO - __main__ -   Epoch = 3, Batch = 850, Batch loss = 0.183941, Avg loss (per batch) = 0.239809\n","05/15/2022 17:08:40 - INFO - __main__ -   Epoch = 3, Batch = 900, Batch loss = 0.000576, Avg loss (per batch) = 0.234423\n","05/15/2022 17:09:19 - INFO - __main__ -   Epoch = 3, Batch = 950, Batch loss = 0.059863, Avg loss (per batch) = 0.232781\n","05/15/2022 17:09:58 - INFO - __main__ -   Epoch = 3, Batch = 1000, Batch loss = 1.001065, Avg loss (per batch) = 0.228899\n","05/15/2022 17:10:37 - INFO - __main__ -   Epoch = 3, Batch = 1050, Batch loss = 0.093374, Avg loss (per batch) = 0.222520\n","05/15/2022 17:11:16 - INFO - __main__ -   Epoch = 3, Batch = 1100, Batch loss = 0.000196, Avg loss (per batch) = 0.212742\n","05/15/2022 17:11:55 - INFO - __main__ -   Epoch = 3, Batch = 1150, Batch loss = 0.011082, Avg loss (per batch) = 0.203512\n","05/15/2022 17:12:34 - INFO - __main__ -   Epoch = 3, Batch = 1200, Batch loss = 0.000129, Avg loss (per batch) = 0.195047\n","05/15/2022 17:13:13 - INFO - __main__ -   Epoch = 3, Batch = 1250, Batch loss = 0.000151, Avg loss (per batch) = 0.187250\n","05/15/2022 17:13:52 - INFO - __main__ -   Epoch = 3, Batch = 1300, Batch loss = 0.000110, Avg loss (per batch) = 0.180054\n","05/15/2022 17:14:30 - INFO - __main__ -   Epoch = 3, Batch = 1350, Batch loss = 0.000129, Avg loss (per batch) = 0.173389\n","05/15/2022 17:15:09 - INFO - __main__ -   Epoch = 3, Batch = 1400, Batch loss = 0.000099, Avg loss (per batch) = 0.167201\n","05/15/2022 17:15:25 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 17:20:01 - INFO - __main__ -   After 3.000000 epoch, Training loss = 0.164731, Training accuracy = 0.957321\n","05/15/2022 17:20:40 - INFO - __main__ -   Epoch = 4, Batch = 50, Batch loss = 0.662860, Avg loss (per batch) = 0.581930\n","05/15/2022 17:21:19 - INFO - __main__ -   Epoch = 4, Batch = 100, Batch loss = 0.000817, Avg loss (per batch) = 0.359442\n","05/15/2022 17:21:58 - INFO - __main__ -   Epoch = 4, Batch = 150, Batch loss = 0.000778, Avg loss (per batch) = 0.283130\n","05/15/2022 17:22:37 - INFO - __main__ -   Epoch = 4, Batch = 200, Batch loss = 0.000944, Avg loss (per batch) = 0.238157\n","05/15/2022 17:23:16 - INFO - __main__ -   Epoch = 4, Batch = 250, Batch loss = 0.043291, Avg loss (per batch) = 0.245077\n","05/15/2022 17:23:55 - INFO - __main__ -   Epoch = 4, Batch = 300, Batch loss = 0.284039, Avg loss (per batch) = 0.217925\n","05/15/2022 17:24:34 - INFO - __main__ -   Epoch = 4, Batch = 350, Batch loss = 0.000889, Avg loss (per batch) = 0.205390\n","05/15/2022 17:25:13 - INFO - __main__ -   Epoch = 4, Batch = 400, Batch loss = 0.000607, Avg loss (per batch) = 0.189372\n","05/15/2022 17:25:52 - INFO - __main__ -   Epoch = 4, Batch = 450, Batch loss = 0.001844, Avg loss (per batch) = 0.184441\n","05/15/2022 17:26:31 - INFO - __main__ -   Epoch = 4, Batch = 500, Batch loss = 0.859274, Avg loss (per batch) = 0.178658\n","05/15/2022 17:27:10 - INFO - __main__ -   Epoch = 4, Batch = 550, Batch loss = 0.003082, Avg loss (per batch) = 0.168589\n","05/15/2022 17:27:49 - INFO - __main__ -   Epoch = 4, Batch = 600, Batch loss = 0.000557, Avg loss (per batch) = 0.163762\n","05/15/2022 17:28:28 - INFO - __main__ -   Epoch = 4, Batch = 650, Batch loss = 0.005091, Avg loss (per batch) = 0.161547\n","05/15/2022 17:29:07 - INFO - __main__ -   Epoch = 4, Batch = 700, Batch loss = 0.000633, Avg loss (per batch) = 0.163710\n","05/15/2022 17:29:46 - INFO - __main__ -   Epoch = 4, Batch = 750, Batch loss = 0.001127, Avg loss (per batch) = 0.158803\n","05/15/2022 17:30:25 - INFO - __main__ -   Epoch = 4, Batch = 800, Batch loss = 0.463850, Avg loss (per batch) = 0.159197\n","05/15/2022 17:31:03 - INFO - __main__ -   Epoch = 4, Batch = 850, Batch loss = 0.002895, Avg loss (per batch) = 0.158605\n","05/15/2022 17:31:42 - INFO - __main__ -   Epoch = 4, Batch = 900, Batch loss = 0.000361, Avg loss (per batch) = 0.155509\n","05/15/2022 17:32:21 - INFO - __main__ -   Epoch = 4, Batch = 950, Batch loss = 0.017817, Avg loss (per batch) = 0.155524\n","05/15/2022 17:33:00 - INFO - __main__ -   Epoch = 4, Batch = 1000, Batch loss = 0.000919, Avg loss (per batch) = 0.152239\n","05/15/2022 17:33:39 - INFO - __main__ -   Epoch = 4, Batch = 1050, Batch loss = 0.411697, Avg loss (per batch) = 0.148726\n","05/15/2022 17:34:18 - INFO - __main__ -   Epoch = 4, Batch = 1100, Batch loss = 0.000164, Avg loss (per batch) = 0.143432\n","05/15/2022 17:34:57 - INFO - __main__ -   Epoch = 4, Batch = 1150, Batch loss = 0.000149, Avg loss (per batch) = 0.137203\n","05/15/2022 17:35:36 - INFO - __main__ -   Epoch = 4, Batch = 1200, Batch loss = 0.000113, Avg loss (per batch) = 0.131492\n","05/15/2022 17:36:14 - INFO - __main__ -   Epoch = 4, Batch = 1250, Batch loss = 0.000109, Avg loss (per batch) = 0.126237\n","05/15/2022 17:36:53 - INFO - __main__ -   Epoch = 4, Batch = 1300, Batch loss = 0.000102, Avg loss (per batch) = 0.121386\n","05/15/2022 17:37:32 - INFO - __main__ -   Epoch = 4, Batch = 1350, Batch loss = 0.000118, Avg loss (per batch) = 0.116896\n","05/15/2022 17:38:11 - INFO - __main__ -   Epoch = 4, Batch = 1400, Batch loss = 0.000087, Avg loss (per batch) = 0.112725\n","05/15/2022 17:38:27 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 17:43:03 - INFO - __main__ -   After 4.000000 epoch, Training loss = 0.111060, Training accuracy = 0.974393\n","05/15/2022 17:43:42 - INFO - __main__ -   Epoch = 5, Batch = 50, Batch loss = 0.573699, Avg loss (per batch) = 0.357305\n","05/15/2022 17:44:21 - INFO - __main__ -   Epoch = 5, Batch = 100, Batch loss = 0.000557, Avg loss (per batch) = 0.216339\n","05/15/2022 17:44:59 - INFO - __main__ -   Epoch = 5, Batch = 150, Batch loss = 0.000380, Avg loss (per batch) = 0.163591\n","05/15/2022 17:45:38 - INFO - __main__ -   Epoch = 5, Batch = 200, Batch loss = 0.000348, Avg loss (per batch) = 0.129714\n","05/15/2022 17:46:17 - INFO - __main__ -   Epoch = 5, Batch = 250, Batch loss = 0.000407, Avg loss (per batch) = 0.130368\n","05/15/2022 17:46:56 - INFO - __main__ -   Epoch = 5, Batch = 300, Batch loss = 0.000344, Avg loss (per batch) = 0.116658\n","05/15/2022 17:47:35 - INFO - __main__ -   Epoch = 5, Batch = 350, Batch loss = 0.000368, Avg loss (per batch) = 0.109295\n","05/15/2022 17:48:14 - INFO - __main__ -   Epoch = 5, Batch = 400, Batch loss = 0.000450, Avg loss (per batch) = 0.106913\n","05/15/2022 17:48:53 - INFO - __main__ -   Epoch = 5, Batch = 450, Batch loss = 0.000627, Avg loss (per batch) = 0.104688\n","05/15/2022 17:49:32 - INFO - __main__ -   Epoch = 5, Batch = 500, Batch loss = 1.035106, Avg loss (per batch) = 0.101564\n","05/15/2022 17:50:11 - INFO - __main__ -   Epoch = 5, Batch = 550, Batch loss = 0.000379, Avg loss (per batch) = 0.095512\n","05/15/2022 17:50:50 - INFO - __main__ -   Epoch = 5, Batch = 600, Batch loss = 0.000377, Avg loss (per batch) = 0.093892\n","05/15/2022 17:51:29 - INFO - __main__ -   Epoch = 5, Batch = 650, Batch loss = 0.000756, Avg loss (per batch) = 0.098002\n","05/15/2022 17:52:07 - INFO - __main__ -   Epoch = 5, Batch = 700, Batch loss = 0.000370, Avg loss (per batch) = 0.102016\n","05/15/2022 17:52:46 - INFO - __main__ -   Epoch = 5, Batch = 750, Batch loss = 0.000532, Avg loss (per batch) = 0.097948\n","05/15/2022 17:53:25 - INFO - __main__ -   Epoch = 5, Batch = 800, Batch loss = 0.683968, Avg loss (per batch) = 0.098150\n","05/15/2022 17:54:04 - INFO - __main__ -   Epoch = 5, Batch = 850, Batch loss = 0.000315, Avg loss (per batch) = 0.101924\n","05/15/2022 17:54:43 - INFO - __main__ -   Epoch = 5, Batch = 900, Batch loss = 0.000202, Avg loss (per batch) = 0.098445\n","05/15/2022 17:55:22 - INFO - __main__ -   Epoch = 5, Batch = 950, Batch loss = 0.003660, Avg loss (per batch) = 0.096284\n","05/15/2022 17:56:01 - INFO - __main__ -   Epoch = 5, Batch = 1000, Batch loss = 0.003946, Avg loss (per batch) = 0.094974\n","05/15/2022 17:56:40 - INFO - __main__ -   Epoch = 5, Batch = 1050, Batch loss = 1.036361, Avg loss (per batch) = 0.093871\n","05/15/2022 17:57:18 - INFO - __main__ -   Epoch = 5, Batch = 1100, Batch loss = 0.000086, Avg loss (per batch) = 0.090516\n","05/15/2022 17:57:57 - INFO - __main__ -   Epoch = 5, Batch = 1150, Batch loss = 0.000110, Avg loss (per batch) = 0.086586\n","05/15/2022 17:58:36 - INFO - __main__ -   Epoch = 5, Batch = 1200, Batch loss = 0.000061, Avg loss (per batch) = 0.082982\n","05/15/2022 17:59:15 - INFO - __main__ -   Epoch = 5, Batch = 1250, Batch loss = 0.000069, Avg loss (per batch) = 0.079666\n","05/15/2022 17:59:54 - INFO - __main__ -   Epoch = 5, Batch = 1300, Batch loss = 0.000053, Avg loss (per batch) = 0.076605\n","05/15/2022 18:00:33 - INFO - __main__ -   Epoch = 5, Batch = 1350, Batch loss = 0.000071, Avg loss (per batch) = 0.073773\n","05/15/2022 18:01:12 - INFO - __main__ -   Epoch = 5, Batch = 1400, Batch loss = 0.000047, Avg loss (per batch) = 0.071142\n","05/15/2022 18:01:28 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 18:06:02 - INFO - __main__ -   After 5.000000 epoch, Training loss = 0.070091, Training accuracy = 0.984073\n","05/15/2022 18:06:41 - INFO - __main__ -   Epoch = 6, Batch = 50, Batch loss = 0.005963, Avg loss (per batch) = 0.377646\n","05/15/2022 18:07:20 - INFO - __main__ -   Epoch = 6, Batch = 100, Batch loss = 0.000279, Avg loss (per batch) = 0.218679\n","05/15/2022 18:07:59 - INFO - __main__ -   Epoch = 6, Batch = 150, Batch loss = 0.000182, Avg loss (per batch) = 0.156346\n","05/15/2022 18:08:38 - INFO - __main__ -   Epoch = 6, Batch = 200, Batch loss = 0.000243, Avg loss (per batch) = 0.125942\n","05/15/2022 18:09:17 - INFO - __main__ -   Epoch = 6, Batch = 250, Batch loss = 0.002423, Avg loss (per batch) = 0.122260\n","05/15/2022 18:09:56 - INFO - __main__ -   Epoch = 6, Batch = 300, Batch loss = 0.000223, Avg loss (per batch) = 0.105628\n","05/15/2022 18:10:34 - INFO - __main__ -   Epoch = 6, Batch = 350, Batch loss = 0.000152, Avg loss (per batch) = 0.099177\n","05/15/2022 18:11:13 - INFO - __main__ -   Epoch = 6, Batch = 400, Batch loss = 0.000206, Avg loss (per batch) = 0.089927\n","05/15/2022 18:11:52 - INFO - __main__ -   Epoch = 6, Batch = 450, Batch loss = 0.000372, Avg loss (per batch) = 0.086722\n","05/15/2022 18:12:31 - INFO - __main__ -   Epoch = 6, Batch = 500, Batch loss = 0.809027, Avg loss (per batch) = 0.084650\n","05/15/2022 18:13:10 - INFO - __main__ -   Epoch = 6, Batch = 550, Batch loss = 0.000220, Avg loss (per batch) = 0.080468\n","05/15/2022 18:13:49 - INFO - __main__ -   Epoch = 6, Batch = 600, Batch loss = 0.000147, Avg loss (per batch) = 0.074620\n","05/15/2022 18:14:28 - INFO - __main__ -   Epoch = 6, Batch = 650, Batch loss = 0.000381, Avg loss (per batch) = 0.076821\n","05/15/2022 18:15:07 - INFO - __main__ -   Epoch = 6, Batch = 700, Batch loss = 0.000237, Avg loss (per batch) = 0.079863\n","05/15/2022 18:15:46 - INFO - __main__ -   Epoch = 6, Batch = 750, Batch loss = 0.000315, Avg loss (per batch) = 0.074820\n","05/15/2022 18:16:25 - INFO - __main__ -   Epoch = 6, Batch = 800, Batch loss = 0.072418, Avg loss (per batch) = 0.073674\n","05/15/2022 18:17:04 - INFO - __main__ -   Epoch = 6, Batch = 850, Batch loss = 0.000199, Avg loss (per batch) = 0.075314\n","05/15/2022 18:17:43 - INFO - __main__ -   Epoch = 6, Batch = 900, Batch loss = 0.000140, Avg loss (per batch) = 0.073322\n","05/15/2022 18:18:22 - INFO - __main__ -   Epoch = 6, Batch = 950, Batch loss = 0.005831, Avg loss (per batch) = 0.075166\n","05/15/2022 18:19:01 - INFO - __main__ -   Epoch = 6, Batch = 1000, Batch loss = 0.000459, Avg loss (per batch) = 0.073904\n","05/15/2022 18:19:40 - INFO - __main__ -   Epoch = 6, Batch = 1050, Batch loss = 0.444247, Avg loss (per batch) = 0.072282\n","05/15/2022 18:20:19 - INFO - __main__ -   Epoch = 6, Batch = 1100, Batch loss = 0.000093, Avg loss (per batch) = 0.072181\n","05/15/2022 18:20:58 - INFO - __main__ -   Epoch = 6, Batch = 1150, Batch loss = 0.000366, Avg loss (per batch) = 0.069046\n","05/15/2022 18:21:37 - INFO - __main__ -   Epoch = 6, Batch = 1200, Batch loss = 0.000059, Avg loss (per batch) = 0.066183\n","05/15/2022 18:22:16 - INFO - __main__ -   Epoch = 6, Batch = 1250, Batch loss = 0.000059, Avg loss (per batch) = 0.063538\n","05/15/2022 18:22:55 - INFO - __main__ -   Epoch = 6, Batch = 1300, Batch loss = 0.000050, Avg loss (per batch) = 0.061098\n","05/15/2022 18:23:34 - INFO - __main__ -   Epoch = 6, Batch = 1350, Batch loss = 0.000091, Avg loss (per batch) = 0.058837\n","05/15/2022 18:24:13 - INFO - __main__ -   Epoch = 6, Batch = 1400, Batch loss = 0.000039, Avg loss (per batch) = 0.056917\n","05/15/2022 18:24:29 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 18:29:05 - INFO - __main__ -   After 6.000000 epoch, Training loss = 0.056077, Training accuracy = 0.987768\n","05/15/2022 18:29:44 - INFO - __main__ -   Epoch = 7, Batch = 50, Batch loss = 0.377607, Avg loss (per batch) = 0.199031\n","05/15/2022 18:30:23 - INFO - __main__ -   Epoch = 7, Batch = 100, Batch loss = 0.000262, Avg loss (per batch) = 0.110808\n","05/15/2022 18:31:02 - INFO - __main__ -   Epoch = 7, Batch = 150, Batch loss = 0.000121, Avg loss (per batch) = 0.082764\n","05/15/2022 18:31:41 - INFO - __main__ -   Epoch = 7, Batch = 200, Batch loss = 0.000147, Avg loss (per batch) = 0.065264\n","05/15/2022 18:32:20 - INFO - __main__ -   Epoch = 7, Batch = 250, Batch loss = 0.000332, Avg loss (per batch) = 0.069780\n","05/15/2022 18:32:59 - INFO - __main__ -   Epoch = 7, Batch = 300, Batch loss = 0.035321, Avg loss (per batch) = 0.060570\n","05/15/2022 18:33:38 - INFO - __main__ -   Epoch = 7, Batch = 350, Batch loss = 0.000116, Avg loss (per batch) = 0.059215\n","05/15/2022 18:34:17 - INFO - __main__ -   Epoch = 7, Batch = 400, Batch loss = 0.000154, Avg loss (per batch) = 0.051991\n","05/15/2022 18:34:56 - INFO - __main__ -   Epoch = 7, Batch = 450, Batch loss = 0.000554, Avg loss (per batch) = 0.051115\n","05/15/2022 18:35:35 - INFO - __main__ -   Epoch = 7, Batch = 500, Batch loss = 0.001327, Avg loss (per batch) = 0.048590\n","05/15/2022 18:36:14 - INFO - __main__ -   Epoch = 7, Batch = 550, Batch loss = 0.000182, Avg loss (per batch) = 0.045692\n","05/15/2022 18:36:53 - INFO - __main__ -   Epoch = 7, Batch = 600, Batch loss = 0.000098, Avg loss (per batch) = 0.047667\n","05/15/2022 18:37:32 - INFO - __main__ -   Epoch = 7, Batch = 650, Batch loss = 0.000218, Avg loss (per batch) = 0.049433\n","05/15/2022 18:38:11 - INFO - __main__ -   Epoch = 7, Batch = 700, Batch loss = 0.000118, Avg loss (per batch) = 0.051315\n","05/15/2022 18:38:50 - INFO - __main__ -   Epoch = 7, Batch = 750, Batch loss = 0.000206, Avg loss (per batch) = 0.048437\n","05/15/2022 18:39:29 - INFO - __main__ -   Epoch = 7, Batch = 800, Batch loss = 0.037008, Avg loss (per batch) = 0.051591\n","05/15/2022 18:40:08 - INFO - __main__ -   Epoch = 7, Batch = 850, Batch loss = 0.000136, Avg loss (per batch) = 0.054583\n","05/15/2022 18:40:47 - INFO - __main__ -   Epoch = 7, Batch = 900, Batch loss = 0.000113, Avg loss (per batch) = 0.052592\n","05/15/2022 18:41:26 - INFO - __main__ -   Epoch = 7, Batch = 950, Batch loss = 0.378998, Avg loss (per batch) = 0.053822\n","05/15/2022 18:42:04 - INFO - __main__ -   Epoch = 7, Batch = 1000, Batch loss = 0.000545, Avg loss (per batch) = 0.054017\n","05/15/2022 18:42:43 - INFO - __main__ -   Epoch = 7, Batch = 1050, Batch loss = 0.410924, Avg loss (per batch) = 0.053313\n","05/15/2022 18:43:22 - INFO - __main__ -   Epoch = 7, Batch = 1100, Batch loss = 0.000070, Avg loss (per batch) = 0.056973\n","05/15/2022 18:44:01 - INFO - __main__ -   Epoch = 7, Batch = 1150, Batch loss = 0.005269, Avg loss (per batch) = 0.055138\n","05/15/2022 18:44:40 - INFO - __main__ -   Epoch = 7, Batch = 1200, Batch loss = 0.000050, Avg loss (per batch) = 0.052848\n","05/15/2022 18:45:19 - INFO - __main__ -   Epoch = 7, Batch = 1250, Batch loss = 0.000051, Avg loss (per batch) = 0.050739\n","05/15/2022 18:45:58 - INFO - __main__ -   Epoch = 7, Batch = 1300, Batch loss = 0.000037, Avg loss (per batch) = 0.048808\n","05/15/2022 18:46:37 - INFO - __main__ -   Epoch = 7, Batch = 1350, Batch loss = 0.000066, Avg loss (per batch) = 0.047002\n","05/15/2022 18:47:16 - INFO - __main__ -   Epoch = 7, Batch = 1400, Batch loss = 0.000035, Avg loss (per batch) = 0.045496\n","05/15/2022 18:47:32 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 18:52:09 - INFO - __main__ -   After 7.000000 epoch, Training loss = 0.044824, Training accuracy = 0.991112\n","05/15/2022 18:52:48 - INFO - __main__ -   Epoch = 8, Batch = 50, Batch loss = 0.007239, Avg loss (per batch) = 0.146382\n","05/15/2022 18:53:27 - INFO - __main__ -   Epoch = 8, Batch = 100, Batch loss = 0.000265, Avg loss (per batch) = 0.094430\n","05/15/2022 18:54:06 - INFO - __main__ -   Epoch = 8, Batch = 150, Batch loss = 0.000084, Avg loss (per batch) = 0.063300\n","05/15/2022 18:54:45 - INFO - __main__ -   Epoch = 8, Batch = 200, Batch loss = 0.001201, Avg loss (per batch) = 0.055942\n","05/15/2022 18:55:24 - INFO - __main__ -   Epoch = 8, Batch = 250, Batch loss = 0.000284, Avg loss (per batch) = 0.062144\n","05/15/2022 18:56:03 - INFO - __main__ -   Epoch = 8, Batch = 300, Batch loss = 0.000125, Avg loss (per batch) = 0.052710\n","05/15/2022 18:56:42 - INFO - __main__ -   Epoch = 8, Batch = 350, Batch loss = 0.000094, Avg loss (per batch) = 0.052727\n","05/15/2022 18:57:21 - INFO - __main__ -   Epoch = 8, Batch = 400, Batch loss = 0.000126, Avg loss (per batch) = 0.047449\n","05/15/2022 18:58:00 - INFO - __main__ -   Epoch = 8, Batch = 450, Batch loss = 0.000202, Avg loss (per batch) = 0.046213\n","05/15/2022 18:58:39 - INFO - __main__ -   Epoch = 8, Batch = 500, Batch loss = 0.000601, Avg loss (per batch) = 0.043424\n","05/15/2022 18:59:18 - INFO - __main__ -   Epoch = 8, Batch = 550, Batch loss = 0.000114, Avg loss (per batch) = 0.043266\n","05/15/2022 18:59:57 - INFO - __main__ -   Epoch = 8, Batch = 600, Batch loss = 0.000068, Avg loss (per batch) = 0.044354\n","05/15/2022 19:00:36 - INFO - __main__ -   Epoch = 8, Batch = 650, Batch loss = 0.000155, Avg loss (per batch) = 0.045665\n","05/15/2022 19:01:15 - INFO - __main__ -   Epoch = 8, Batch = 700, Batch loss = 0.000113, Avg loss (per batch) = 0.044500\n","05/15/2022 19:01:54 - INFO - __main__ -   Epoch = 8, Batch = 750, Batch loss = 0.000211, Avg loss (per batch) = 0.042305\n","05/15/2022 19:02:32 - INFO - __main__ -   Epoch = 8, Batch = 800, Batch loss = 0.002463, Avg loss (per batch) = 0.041786\n","05/15/2022 19:03:12 - INFO - __main__ -   Epoch = 8, Batch = 850, Batch loss = 0.000092, Avg loss (per batch) = 0.040753\n","05/15/2022 19:03:51 - INFO - __main__ -   Epoch = 8, Batch = 900, Batch loss = 0.000065, Avg loss (per batch) = 0.040084\n","05/15/2022 19:04:30 - INFO - __main__ -   Epoch = 8, Batch = 950, Batch loss = 0.000315, Avg loss (per batch) = 0.039087\n","05/15/2022 19:05:09 - INFO - __main__ -   Epoch = 8, Batch = 1000, Batch loss = 0.000122, Avg loss (per batch) = 0.037356\n","05/15/2022 19:05:48 - INFO - __main__ -   Epoch = 8, Batch = 1050, Batch loss = 0.147795, Avg loss (per batch) = 0.036650\n","05/15/2022 19:06:27 - INFO - __main__ -   Epoch = 8, Batch = 1100, Batch loss = 0.000073, Avg loss (per batch) = 0.040743\n","05/15/2022 19:07:06 - INFO - __main__ -   Epoch = 8, Batch = 1150, Batch loss = 0.000039, Avg loss (per batch) = 0.041261\n","05/15/2022 19:07:45 - INFO - __main__ -   Epoch = 8, Batch = 1200, Batch loss = 0.000049, Avg loss (per batch) = 0.040264\n","05/15/2022 19:08:24 - INFO - __main__ -   Epoch = 8, Batch = 1250, Batch loss = 0.000057, Avg loss (per batch) = 0.038760\n","05/15/2022 19:09:03 - INFO - __main__ -   Epoch = 8, Batch = 1300, Batch loss = 0.000035, Avg loss (per batch) = 0.038140\n","05/15/2022 19:09:42 - INFO - __main__ -   Epoch = 8, Batch = 1350, Batch loss = 0.002181, Avg loss (per batch) = 0.036754\n","05/15/2022 19:10:21 - INFO - __main__ -   Epoch = 8, Batch = 1400, Batch loss = 0.000031, Avg loss (per batch) = 0.035443\n","05/15/2022 19:10:37 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 19:15:13 - INFO - __main__ -   After 8.000000 epoch, Training loss = 0.034920, Training accuracy = 0.992696\n","05/15/2022 19:15:52 - INFO - __main__ -   Epoch = 9, Batch = 50, Batch loss = 0.000410, Avg loss (per batch) = 0.105092\n","05/15/2022 19:16:31 - INFO - __main__ -   Epoch = 9, Batch = 100, Batch loss = 0.000075, Avg loss (per batch) = 0.079360\n","05/15/2022 19:17:10 - INFO - __main__ -   Epoch = 9, Batch = 150, Batch loss = 0.000061, Avg loss (per batch) = 0.058066\n","05/15/2022 19:17:49 - INFO - __main__ -   Epoch = 9, Batch = 200, Batch loss = 0.000043, Avg loss (per batch) = 0.043838\n","05/15/2022 19:18:28 - INFO - __main__ -   Epoch = 9, Batch = 250, Batch loss = 0.000140, Avg loss (per batch) = 0.041831\n","05/15/2022 19:19:07 - INFO - __main__ -   Epoch = 9, Batch = 300, Batch loss = 0.000060, Avg loss (per batch) = 0.035639\n","05/15/2022 19:19:46 - INFO - __main__ -   Epoch = 9, Batch = 350, Batch loss = 0.000097, Avg loss (per batch) = 0.037034\n","05/15/2022 19:20:25 - INFO - __main__ -   Epoch = 9, Batch = 400, Batch loss = 0.000082, Avg loss (per batch) = 0.034551\n","05/15/2022 19:21:04 - INFO - __main__ -   Epoch = 9, Batch = 450, Batch loss = 0.000142, Avg loss (per batch) = 0.032634\n","05/15/2022 19:21:43 - INFO - __main__ -   Epoch = 9, Batch = 500, Batch loss = 0.467252, Avg loss (per batch) = 0.030388\n","05/15/2022 19:22:22 - INFO - __main__ -   Epoch = 9, Batch = 550, Batch loss = 0.000063, Avg loss (per batch) = 0.032144\n","05/15/2022 19:23:01 - INFO - __main__ -   Epoch = 9, Batch = 600, Batch loss = 0.000413, Avg loss (per batch) = 0.029752\n","05/15/2022 19:23:40 - INFO - __main__ -   Epoch = 9, Batch = 650, Batch loss = 0.002311, Avg loss (per batch) = 0.029513\n","05/15/2022 19:24:19 - INFO - __main__ -   Epoch = 9, Batch = 700, Batch loss = 0.000056, Avg loss (per batch) = 0.028708\n","05/15/2022 19:24:58 - INFO - __main__ -   Epoch = 9, Batch = 750, Batch loss = 0.000096, Avg loss (per batch) = 0.026853\n","05/15/2022 19:25:37 - INFO - __main__ -   Epoch = 9, Batch = 800, Batch loss = 0.000635, Avg loss (per batch) = 0.027466\n","05/15/2022 19:26:16 - INFO - __main__ -   Epoch = 9, Batch = 850, Batch loss = 0.000069, Avg loss (per batch) = 0.026837\n","05/15/2022 19:26:55 - INFO - __main__ -   Epoch = 9, Batch = 900, Batch loss = 0.000044, Avg loss (per batch) = 0.026030\n","05/15/2022 19:27:34 - INFO - __main__ -   Epoch = 9, Batch = 950, Batch loss = 0.000120, Avg loss (per batch) = 0.024695\n","05/15/2022 19:28:13 - INFO - __main__ -   Epoch = 9, Batch = 1000, Batch loss = 0.000203, Avg loss (per batch) = 0.023594\n","05/15/2022 19:28:52 - INFO - __main__ -   Epoch = 9, Batch = 1050, Batch loss = 0.375994, Avg loss (per batch) = 0.024066\n","05/15/2022 19:29:31 - INFO - __main__ -   Epoch = 9, Batch = 1100, Batch loss = 0.154179, Avg loss (per batch) = 0.028526\n","05/15/2022 19:30:10 - INFO - __main__ -   Epoch = 9, Batch = 1150, Batch loss = 0.013585, Avg loss (per batch) = 0.029094\n","05/15/2022 19:30:49 - INFO - __main__ -   Epoch = 9, Batch = 1200, Batch loss = 0.000028, Avg loss (per batch) = 0.028997\n","05/15/2022 19:31:28 - INFO - __main__ -   Epoch = 9, Batch = 1250, Batch loss = 0.000040, Avg loss (per batch) = 0.028103\n","05/15/2022 19:32:07 - INFO - __main__ -   Epoch = 9, Batch = 1300, Batch loss = 0.000024, Avg loss (per batch) = 0.028883\n","05/15/2022 19:32:46 - INFO - __main__ -   Epoch = 9, Batch = 1350, Batch loss = 0.000074, Avg loss (per batch) = 0.027886\n","05/15/2022 19:33:25 - INFO - __main__ -   Epoch = 9, Batch = 1400, Batch loss = 0.000022, Avg loss (per batch) = 0.026996\n","05/15/2022 19:33:41 - INFO - __main__ -   Creating a checkpoint.\n","05/15/2022 19:38:18 - INFO - __main__ -   After 9.000000 epoch, Training loss = 0.026597, Training accuracy = 0.994104\n","05/15/2022 19:38:57 - INFO - __main__ -   Epoch = 10, Batch = 50, Batch loss = 0.012680, Avg loss (per batch) = 0.034903\n","05/15/2022 19:39:36 - INFO - __main__ -   Epoch = 10, Batch = 100, Batch loss = 0.000142, Avg loss (per batch) = 0.054306\n","05/15/2022 19:40:15 - INFO - __main__ -   Epoch = 10, Batch = 150, Batch loss = 0.000060, Avg loss (per batch) = 0.038428\n","05/15/2022 19:40:54 - INFO - __main__ -   Epoch = 10, Batch = 200, Batch loss = 0.000038, Avg loss (per batch) = 0.035603\n","05/15/2022 19:41:33 - INFO - __main__ -   Epoch = 10, Batch = 250, Batch loss = 0.000095, Avg loss (per batch) = 0.038108\n","05/15/2022 19:42:12 - INFO - __main__ -   Epoch = 10, Batch = 300, Batch loss = 0.000078, Avg loss (per batch) = 0.034612\n","05/15/2022 19:42:51 - INFO - __main__ -   Epoch = 10, Batch = 350, Batch loss = 0.000064, Avg loss (per batch) = 0.035587\n","05/15/2022 19:43:30 - INFO - __main__ -   Epoch = 10, Batch = 400, Batch loss = 0.000146, Avg loss (per batch) = 0.033626\n","05/15/2022 19:44:09 - INFO - __main__ -   Epoch = 10, Batch = 450, Batch loss = 0.443664, Avg loss (per batch) = 0.032580\n","05/15/2022 19:44:48 - INFO - __main__ -   Epoch = 10, Batch = 500, Batch loss = 0.000133, Avg loss (per batch) = 0.032556\n","05/15/2022 19:45:27 - INFO - __main__ -   Epoch = 10, Batch = 550, Batch loss = 0.000044, Avg loss (per batch) = 0.030805\n","05/15/2022 19:46:06 - INFO - __main__ -   Epoch = 10, Batch = 600, Batch loss = 0.000043, Avg loss (per batch) = 0.028964\n","05/15/2022 19:46:45 - INFO - __main__ -   Epoch = 10, Batch = 650, Batch loss = 0.000073, Avg loss (per batch) = 0.028330\n","05/15/2022 19:47:24 - INFO - __main__ -   Epoch = 10, Batch = 700, Batch loss = 0.000043, Avg loss (per batch) = 0.027903\n","05/15/2022 19:48:03 - INFO - __main__ -   Epoch = 10, Batch = 750, Batch loss = 0.000074, Avg loss (per batch) = 0.026135\n","05/15/2022 19:48:42 - INFO - __main__ -   Epoch = 10, Batch = 800, Batch loss = 0.000085, Avg loss (per batch) = 0.025314\n","05/15/2022 19:49:21 - INFO - __main__ -   Epoch = 10, Batch = 850, Batch loss = 0.000075, Avg loss (per batch) = 0.024374\n","05/15/2022 19:50:00 - INFO - __main__ -   Epoch = 10, Batch = 900, Batch loss = 0.000060, Avg loss (per batch) = 0.023986\n","05/15/2022 19:50:39 - INFO - __main__ -   Epoch = 10, Batch = 950, Batch loss = 0.000169, Avg loss (per batch) = 0.022784\n","05/15/2022 19:51:18 - INFO - __main__ -   Epoch = 10, Batch = 1000, Batch loss = 0.001345, Avg loss (per batch) = 0.022277\n","05/15/2022 19:51:57 - INFO - __main__ -   Epoch = 10, Batch = 1050, Batch loss = 0.001692, Avg loss (per batch) = 0.021375\n","05/15/2022 19:52:36 - INFO - __main__ -   Epoch = 10, Batch = 1100, Batch loss = 0.804299, Avg loss (per batch) = 0.025565\n","05/15/2022 19:53:15 - INFO - __main__ -   Epoch = 10, Batch = 1150, Batch loss = 0.000191, Avg loss (per batch) = 0.033708\n","05/15/2022 19:53:55 - INFO - __main__ -   Epoch = 10, Batch = 1200, Batch loss = 0.000039, Avg loss (per batch) = 0.041928\n","05/15/2022 19:54:33 - INFO - __main__ -   Epoch = 10, Batch = 1250, Batch loss = 0.820100, Avg loss (per batch) = 0.050116\n","05/15/2022 19:55:12 - INFO - __main__ -   Epoch = 10, Batch = 1300, Batch loss = 0.000214, Avg loss (per batch) = 0.062576\n"]}],"source":["# train set : \"data/sentihood/bert-sentclass-delete50none/train_QA_M.tsv\"\n","!python3 bert_sentclass.py --num_train_epochs 10 --max_seq_length 512 --batch_size 8"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2534,"status":"ok","timestamp":1652645245099,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"LIcHSCp0KkW5","outputId":"e4f9b08f-f3da-4b19-e3fd-baefd393fb46"},"outputs":[{"output_type":"stream","name":"stdout","text":["aspect_strict_Acc = 0.5178958785249458\n","aspect_Macro_F1 = 0.09015055323780156\n","aspect_Macro_AUC = 0.9318675900590828\n","sentiment_Acc = 0.8821070234113713\n","sentiment_Macro_AUC = 0.9073139602142625\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_6.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1474,"status":"ok","timestamp":1652645249335,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"Yib74npgiT01","outputId":"f5750640-139f-4a69-b465-796ac7550b86"},"outputs":[{"output_type":"stream","name":"stdout","text":["aspect_strict_Acc = 0.5515184381778742\n","aspect_Macro_F1 = 0.18991551931660824\n","aspect_Macro_AUC = 0.9277902018121561\n","sentiment_Acc = 0.8770903010033445\n","sentiment_Macro_AUC = 0.9194747682008875\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_7.txt "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1347,"status":"ok","timestamp":1652645252530,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"pg-yJWk8nWpj","outputId":"9d97d8a6-a6e1-40f6-eb4e-0ac6f1b0c1fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["aspect_strict_Acc = 0.6084598698481561\n","aspect_Macro_F1 = 0.321507724339559\n","aspect_Macro_AUC = 0.9381756592721929\n","sentiment_Acc = 0.8862876254180602\n","sentiment_Macro_AUC = 0.9241461869029939\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_8.txt"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1402,"status":"ok","timestamp":1652645256365,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"33uR2RG2Q7S4","outputId":"24328098-e055-4daf-ce30-65cd9b14a0ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["aspect_strict_Acc = 0.6800433839479393\n","aspect_Macro_F1 = 0.4990960743104157\n","aspect_Macro_AUC = 0.9399020777442976\n","sentiment_Acc = 0.9038461538461539\n","sentiment_Macro_AUC = 0.945870970149929\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_9.txt"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"launcher_notebook_bert_delete50none.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}