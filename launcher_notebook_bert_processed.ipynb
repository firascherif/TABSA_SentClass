{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17125,"status":"ok","timestamp":1652446078979,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"zBPFB9bu2haM","outputId":"f06ca1f1-cf81-46e0-a564-565e8bd04ed2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktyxvV4XeDjG"},"outputs":[],"source":["cd drive/MyDrive/Colab_notebooks/BERTSentclass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znO7aUwk6yYX"},"outputs":[],"source":["#!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1652446237894,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"FJs0c95udMWg","outputId":"d19c606a-1dfd-4636-822a-3019983206a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["bert_sentclass.py  \u001b[0m\u001b[01;34mdata\u001b[0m/                                        \u001b[01;34mresults\u001b[0m/\n","bert_sentiment.py  launcher_notebook_bert_data_processed.ipynb\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21301459,"status":"ok","timestamp":1652467691308,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"O-17rYxwgpOU","outputId":"96e4eb0c-9933-4f4f-8ce7-493362bea358"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading: 100% 226k/226k [00:00<00:00, 1.97MB/s]\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.1kB/s]\n","Downloading: 100% 570/570 [00:00<00:00, 524kB/s]\n","Downloading: 100% 420M/420M [00:10<00:00, 43.7MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","05/13/2022 12:55:08 - INFO - __main__ -   Epoch = 1, Batch = 50, Batch loss = 1.061344, Avg loss (per batch) = 0.678368\n","05/13/2022 12:56:22 - INFO - __main__ -   Epoch = 1, Batch = 100, Batch loss = 0.105127, Avg loss (per batch) = 0.623918\n","05/13/2022 12:57:35 - INFO - __main__ -   Epoch = 1, Batch = 150, Batch loss = 1.463235, Avg loss (per batch) = 0.633133\n","05/13/2022 12:58:48 - INFO - __main__ -   Epoch = 1, Batch = 200, Batch loss = 0.024058, Avg loss (per batch) = 0.602988\n","05/13/2022 13:00:01 - INFO - __main__ -   Epoch = 1, Batch = 250, Batch loss = 0.571257, Avg loss (per batch) = 0.599779\n","05/13/2022 13:01:14 - INFO - __main__ -   Epoch = 1, Batch = 300, Batch loss = 0.008974, Avg loss (per batch) = 0.580229\n","05/13/2022 13:02:28 - INFO - __main__ -   Epoch = 1, Batch = 350, Batch loss = 0.671401, Avg loss (per batch) = 0.574465\n","05/13/2022 13:03:42 - INFO - __main__ -   Epoch = 1, Batch = 400, Batch loss = 1.292208, Avg loss (per batch) = 0.564066\n","05/13/2022 13:04:55 - INFO - __main__ -   Epoch = 1, Batch = 450, Batch loss = 0.003874, Avg loss (per batch) = 0.545885\n","05/13/2022 13:06:09 - INFO - __main__ -   Epoch = 1, Batch = 500, Batch loss = 0.303263, Avg loss (per batch) = 0.535955\n","05/13/2022 13:07:23 - INFO - __main__ -   Epoch = 1, Batch = 550, Batch loss = 0.244121, Avg loss (per batch) = 0.536870\n","05/13/2022 13:08:36 - INFO - __main__ -   Epoch = 1, Batch = 600, Batch loss = 1.272329, Avg loss (per batch) = 0.531777\n","05/13/2022 13:09:50 - INFO - __main__ -   Epoch = 1, Batch = 650, Batch loss = 0.214228, Avg loss (per batch) = 0.529227\n","05/13/2022 13:11:03 - INFO - __main__ -   Epoch = 1, Batch = 700, Batch loss = 0.708255, Avg loss (per batch) = 0.520616\n","05/13/2022 13:12:17 - INFO - __main__ -   Epoch = 1, Batch = 750, Batch loss = 1.134950, Avg loss (per batch) = 0.515491\n","05/13/2022 13:13:31 - INFO - __main__ -   Epoch = 1, Batch = 800, Batch loss = 0.004276, Avg loss (per batch) = 0.510635\n","05/13/2022 13:14:44 - INFO - __main__ -   Epoch = 1, Batch = 850, Batch loss = 0.664549, Avg loss (per batch) = 0.500801\n","05/13/2022 13:15:58 - INFO - __main__ -   Epoch = 1, Batch = 900, Batch loss = 0.002036, Avg loss (per batch) = 0.489769\n","05/13/2022 13:17:11 - INFO - __main__ -   Epoch = 1, Batch = 950, Batch loss = 1.337663, Avg loss (per batch) = 0.490532\n","05/13/2022 13:18:25 - INFO - __main__ -   Epoch = 1, Batch = 1000, Batch loss = 0.165519, Avg loss (per batch) = 0.500241\n","05/13/2022 13:19:39 - INFO - __main__ -   Epoch = 1, Batch = 1050, Batch loss = 0.457865, Avg loss (per batch) = 0.501721\n","05/13/2022 13:20:52 - INFO - __main__ -   Epoch = 1, Batch = 1100, Batch loss = 0.004104, Avg loss (per batch) = 0.497315\n","05/13/2022 13:21:15 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 13:29:27 - INFO - __main__ -   After 1.000000 epoch, Training loss = 0.505141, Training accuracy = 0.817998\n","05/13/2022 13:30:40 - INFO - __main__ -   Epoch = 2, Batch = 50, Batch loss = 0.066951, Avg loss (per batch) = 0.376902\n","05/13/2022 13:31:54 - INFO - __main__ -   Epoch = 2, Batch = 100, Batch loss = 0.057649, Avg loss (per batch) = 0.338533\n","05/13/2022 13:33:08 - INFO - __main__ -   Epoch = 2, Batch = 150, Batch loss = 0.396739, Avg loss (per batch) = 0.367806\n","05/13/2022 13:34:21 - INFO - __main__ -   Epoch = 2, Batch = 200, Batch loss = 0.002300, Avg loss (per batch) = 0.373299\n","05/13/2022 13:35:35 - INFO - __main__ -   Epoch = 2, Batch = 250, Batch loss = 0.008902, Avg loss (per batch) = 0.354369\n","05/13/2022 13:36:48 - INFO - __main__ -   Epoch = 2, Batch = 300, Batch loss = 0.003350, Avg loss (per batch) = 0.337454\n","05/13/2022 13:38:02 - INFO - __main__ -   Epoch = 2, Batch = 350, Batch loss = 0.379194, Avg loss (per batch) = 0.330504\n","05/13/2022 13:39:16 - INFO - __main__ -   Epoch = 2, Batch = 400, Batch loss = 1.464247, Avg loss (per batch) = 0.321682\n","05/13/2022 13:40:29 - INFO - __main__ -   Epoch = 2, Batch = 450, Batch loss = 0.000797, Avg loss (per batch) = 0.309921\n","05/13/2022 13:41:43 - INFO - __main__ -   Epoch = 2, Batch = 500, Batch loss = 0.214448, Avg loss (per batch) = 0.298110\n","05/13/2022 13:42:57 - INFO - __main__ -   Epoch = 2, Batch = 550, Batch loss = 0.001231, Avg loss (per batch) = 0.294924\n","05/13/2022 13:44:10 - INFO - __main__ -   Epoch = 2, Batch = 600, Batch loss = 1.280158, Avg loss (per batch) = 0.291446\n","05/13/2022 13:45:24 - INFO - __main__ -   Epoch = 2, Batch = 650, Batch loss = 0.003094, Avg loss (per batch) = 0.288788\n","05/13/2022 13:46:38 - INFO - __main__ -   Epoch = 2, Batch = 700, Batch loss = 0.104324, Avg loss (per batch) = 0.277108\n","05/13/2022 13:47:51 - INFO - __main__ -   Epoch = 2, Batch = 750, Batch loss = 0.109353, Avg loss (per batch) = 0.274919\n","05/13/2022 13:49:05 - INFO - __main__ -   Epoch = 2, Batch = 800, Batch loss = 0.000502, Avg loss (per batch) = 0.276522\n","05/13/2022 13:50:18 - INFO - __main__ -   Epoch = 2, Batch = 850, Batch loss = 0.931214, Avg loss (per batch) = 0.272730\n","05/13/2022 13:51:32 - INFO - __main__ -   Epoch = 2, Batch = 900, Batch loss = 0.444653, Avg loss (per batch) = 0.267648\n","05/13/2022 13:52:45 - INFO - __main__ -   Epoch = 2, Batch = 950, Batch loss = 0.012435, Avg loss (per batch) = 0.267382\n","05/13/2022 13:53:59 - INFO - __main__ -   Epoch = 2, Batch = 1000, Batch loss = 0.002973, Avg loss (per batch) = 0.274530\n","05/13/2022 13:55:13 - INFO - __main__ -   Epoch = 2, Batch = 1050, Batch loss = 0.004156, Avg loss (per batch) = 0.271309\n","05/13/2022 13:56:26 - INFO - __main__ -   Epoch = 2, Batch = 1100, Batch loss = 0.002887, Avg loss (per batch) = 0.272295\n","05/13/2022 13:56:49 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 14:05:01 - INFO - __main__ -   After 2.000000 epoch, Training loss = 0.276523, Training accuracy = 0.920430\n","05/13/2022 14:06:14 - INFO - __main__ -   Epoch = 3, Batch = 50, Batch loss = 0.103102, Avg loss (per batch) = 0.202610\n","05/13/2022 14:07:28 - INFO - __main__ -   Epoch = 3, Batch = 100, Batch loss = 0.000342, Avg loss (per batch) = 0.176898\n","05/13/2022 14:08:41 - INFO - __main__ -   Epoch = 3, Batch = 150, Batch loss = 0.013404, Avg loss (per batch) = 0.217595\n","05/13/2022 14:09:55 - INFO - __main__ -   Epoch = 3, Batch = 200, Batch loss = 0.000316, Avg loss (per batch) = 0.228222\n","05/13/2022 14:11:09 - INFO - __main__ -   Epoch = 3, Batch = 250, Batch loss = 0.003164, Avg loss (per batch) = 0.205670\n","05/13/2022 14:12:22 - INFO - __main__ -   Epoch = 3, Batch = 300, Batch loss = 0.001797, Avg loss (per batch) = 0.195202\n","05/13/2022 14:13:36 - INFO - __main__ -   Epoch = 3, Batch = 350, Batch loss = 0.468213, Avg loss (per batch) = 0.187835\n","05/13/2022 14:14:49 - INFO - __main__ -   Epoch = 3, Batch = 400, Batch loss = 0.545426, Avg loss (per batch) = 0.179193\n","05/13/2022 14:16:03 - INFO - __main__ -   Epoch = 3, Batch = 450, Batch loss = 0.000272, Avg loss (per batch) = 0.182247\n","05/13/2022 14:17:17 - INFO - __main__ -   Epoch = 3, Batch = 500, Batch loss = 0.005569, Avg loss (per batch) = 0.167532\n","05/13/2022 14:18:30 - INFO - __main__ -   Epoch = 3, Batch = 550, Batch loss = 0.000200, Avg loss (per batch) = 0.168357\n","05/13/2022 14:19:44 - INFO - __main__ -   Epoch = 3, Batch = 600, Batch loss = 0.718274, Avg loss (per batch) = 0.169273\n","05/13/2022 14:20:57 - INFO - __main__ -   Epoch = 3, Batch = 650, Batch loss = 0.005428, Avg loss (per batch) = 0.166157\n","05/13/2022 14:22:11 - INFO - __main__ -   Epoch = 3, Batch = 700, Batch loss = 0.008595, Avg loss (per batch) = 0.158057\n","05/13/2022 14:23:24 - INFO - __main__ -   Epoch = 3, Batch = 750, Batch loss = 0.002121, Avg loss (per batch) = 0.154196\n","05/13/2022 14:24:37 - INFO - __main__ -   Epoch = 3, Batch = 800, Batch loss = 0.000210, Avg loss (per batch) = 0.155126\n","05/13/2022 14:25:50 - INFO - __main__ -   Epoch = 3, Batch = 850, Batch loss = 0.176638, Avg loss (per batch) = 0.150329\n","05/13/2022 14:27:03 - INFO - __main__ -   Epoch = 3, Batch = 900, Batch loss = 0.000160, Avg loss (per batch) = 0.148328\n","05/13/2022 14:28:17 - INFO - __main__ -   Epoch = 3, Batch = 950, Batch loss = 0.001772, Avg loss (per batch) = 0.143867\n","05/13/2022 14:29:30 - INFO - __main__ -   Epoch = 3, Batch = 1000, Batch loss = 0.000545, Avg loss (per batch) = 0.152525\n","05/13/2022 14:30:43 - INFO - __main__ -   Epoch = 3, Batch = 1050, Batch loss = 0.112871, Avg loss (per batch) = 0.154395\n","05/13/2022 14:31:56 - INFO - __main__ -   Epoch = 3, Batch = 1100, Batch loss = 0.000120, Avg loss (per batch) = 0.157486\n","05/13/2022 14:32:19 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 14:40:26 - INFO - __main__ -   After 3.000000 epoch, Training loss = 0.161718, Training accuracy = 0.959767\n","05/13/2022 14:41:40 - INFO - __main__ -   Epoch = 4, Batch = 50, Batch loss = 0.001165, Avg loss (per batch) = 0.068434\n","05/13/2022 14:42:53 - INFO - __main__ -   Epoch = 4, Batch = 100, Batch loss = 0.000111, Avg loss (per batch) = 0.048593\n","05/13/2022 14:44:06 - INFO - __main__ -   Epoch = 4, Batch = 150, Batch loss = 0.097375, Avg loss (per batch) = 0.100912\n","05/13/2022 14:45:19 - INFO - __main__ -   Epoch = 4, Batch = 200, Batch loss = 0.000095, Avg loss (per batch) = 0.125200\n","05/13/2022 14:46:33 - INFO - __main__ -   Epoch = 4, Batch = 250, Batch loss = 0.000769, Avg loss (per batch) = 0.107391\n","05/13/2022 14:47:46 - INFO - __main__ -   Epoch = 4, Batch = 300, Batch loss = 0.000096, Avg loss (per batch) = 0.099473\n","05/13/2022 14:49:00 - INFO - __main__ -   Epoch = 4, Batch = 350, Batch loss = 0.005461, Avg loss (per batch) = 0.097460\n","05/13/2022 14:50:14 - INFO - __main__ -   Epoch = 4, Batch = 400, Batch loss = 0.107091, Avg loss (per batch) = 0.091773\n","05/13/2022 14:51:28 - INFO - __main__ -   Epoch = 4, Batch = 450, Batch loss = 0.000124, Avg loss (per batch) = 0.087227\n","05/13/2022 14:52:42 - INFO - __main__ -   Epoch = 4, Batch = 500, Batch loss = 0.002245, Avg loss (per batch) = 0.080735\n","05/13/2022 14:53:56 - INFO - __main__ -   Epoch = 4, Batch = 550, Batch loss = 0.000074, Avg loss (per batch) = 0.086747\n","05/13/2022 14:55:10 - INFO - __main__ -   Epoch = 4, Batch = 600, Batch loss = 0.569726, Avg loss (per batch) = 0.088604\n","05/13/2022 14:56:23 - INFO - __main__ -   Epoch = 4, Batch = 650, Batch loss = 0.000692, Avg loss (per batch) = 0.093378\n","05/13/2022 14:57:37 - INFO - __main__ -   Epoch = 4, Batch = 700, Batch loss = 0.001641, Avg loss (per batch) = 0.087472\n","05/13/2022 14:58:50 - INFO - __main__ -   Epoch = 4, Batch = 750, Batch loss = 0.214289, Avg loss (per batch) = 0.089178\n","05/13/2022 15:00:04 - INFO - __main__ -   Epoch = 4, Batch = 800, Batch loss = 0.000073, Avg loss (per batch) = 0.086971\n","05/13/2022 15:01:18 - INFO - __main__ -   Epoch = 4, Batch = 850, Batch loss = 0.000934, Avg loss (per batch) = 0.085732\n","05/13/2022 15:02:31 - INFO - __main__ -   Epoch = 4, Batch = 900, Batch loss = 0.467902, Avg loss (per batch) = 0.083930\n","05/13/2022 15:03:45 - INFO - __main__ -   Epoch = 4, Batch = 950, Batch loss = 0.000673, Avg loss (per batch) = 0.080441\n","05/13/2022 15:04:58 - INFO - __main__ -   Epoch = 4, Batch = 1000, Batch loss = 0.000692, Avg loss (per batch) = 0.087973\n","05/13/2022 15:06:12 - INFO - __main__ -   Epoch = 4, Batch = 1050, Batch loss = 0.620212, Avg loss (per batch) = 0.091232\n","05/13/2022 15:07:26 - INFO - __main__ -   Epoch = 4, Batch = 1100, Batch loss = 0.000055, Avg loss (per batch) = 0.092343\n","05/13/2022 15:07:48 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 15:15:55 - INFO - __main__ -   After 4.000000 epoch, Training loss = 0.091249, Training accuracy = 0.977138\n","05/13/2022 15:17:08 - INFO - __main__ -   Epoch = 5, Batch = 50, Batch loss = 0.000520, Avg loss (per batch) = 0.037850\n","05/13/2022 15:18:21 - INFO - __main__ -   Epoch = 5, Batch = 100, Batch loss = 0.000048, Avg loss (per batch) = 0.038275\n","05/13/2022 15:19:34 - INFO - __main__ -   Epoch = 5, Batch = 150, Batch loss = 0.005868, Avg loss (per batch) = 0.082964\n","05/13/2022 15:20:47 - INFO - __main__ -   Epoch = 5, Batch = 200, Batch loss = 0.000052, Avg loss (per batch) = 0.076198\n","05/13/2022 15:22:00 - INFO - __main__ -   Epoch = 5, Batch = 250, Batch loss = 0.000188, Avg loss (per batch) = 0.064182\n","05/13/2022 15:23:14 - INFO - __main__ -   Epoch = 5, Batch = 300, Batch loss = 0.000105, Avg loss (per batch) = 0.057120\n","05/13/2022 15:24:27 - INFO - __main__ -   Epoch = 5, Batch = 350, Batch loss = 0.317543, Avg loss (per batch) = 0.054382\n","05/13/2022 15:25:40 - INFO - __main__ -   Epoch = 5, Batch = 400, Batch loss = 0.000694, Avg loss (per batch) = 0.052919\n","05/13/2022 15:26:53 - INFO - __main__ -   Epoch = 5, Batch = 450, Batch loss = 0.000059, Avg loss (per batch) = 0.055029\n","05/13/2022 15:28:07 - INFO - __main__ -   Epoch = 5, Batch = 500, Batch loss = 0.000451, Avg loss (per batch) = 0.050888\n","05/13/2022 15:29:20 - INFO - __main__ -   Epoch = 5, Batch = 550, Batch loss = 0.000047, Avg loss (per batch) = 0.049791\n","05/13/2022 15:30:33 - INFO - __main__ -   Epoch = 5, Batch = 600, Batch loss = 0.000640, Avg loss (per batch) = 0.052975\n","05/13/2022 15:31:46 - INFO - __main__ -   Epoch = 5, Batch = 650, Batch loss = 0.000266, Avg loss (per batch) = 0.052753\n","05/13/2022 15:32:59 - INFO - __main__ -   Epoch = 5, Batch = 700, Batch loss = 0.000563, Avg loss (per batch) = 0.049217\n","05/13/2022 15:34:13 - INFO - __main__ -   Epoch = 5, Batch = 750, Batch loss = 0.000812, Avg loss (per batch) = 0.049942\n","05/13/2022 15:35:26 - INFO - __main__ -   Epoch = 5, Batch = 800, Batch loss = 0.000035, Avg loss (per batch) = 0.049922\n","05/13/2022 15:36:39 - INFO - __main__ -   Epoch = 5, Batch = 850, Batch loss = 0.000361, Avg loss (per batch) = 0.050381\n","05/13/2022 15:37:52 - INFO - __main__ -   Epoch = 5, Batch = 900, Batch loss = 0.000055, Avg loss (per batch) = 0.052588\n","05/13/2022 15:39:05 - INFO - __main__ -   Epoch = 5, Batch = 950, Batch loss = 0.000352, Avg loss (per batch) = 0.051215\n","05/13/2022 15:40:19 - INFO - __main__ -   Epoch = 5, Batch = 1000, Batch loss = 0.000194, Avg loss (per batch) = 0.060558\n","05/13/2022 15:41:32 - INFO - __main__ -   Epoch = 5, Batch = 1050, Batch loss = 0.518494, Avg loss (per batch) = 0.067142\n","05/13/2022 15:42:45 - INFO - __main__ -   Epoch = 5, Batch = 1100, Batch loss = 0.000042, Avg loss (per batch) = 0.067283\n","05/13/2022 15:43:08 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 15:51:18 - INFO - __main__ -   After 5.000000 epoch, Training loss = 0.066783, Training accuracy = 0.985879\n","05/13/2022 15:52:31 - INFO - __main__ -   Epoch = 6, Batch = 50, Batch loss = 0.000708, Avg loss (per batch) = 0.037049\n","05/13/2022 15:53:45 - INFO - __main__ -   Epoch = 6, Batch = 100, Batch loss = 0.000031, Avg loss (per batch) = 0.052253\n","05/13/2022 15:54:58 - INFO - __main__ -   Epoch = 6, Batch = 150, Batch loss = 0.000362, Avg loss (per batch) = 0.056928\n","05/13/2022 15:56:11 - INFO - __main__ -   Epoch = 6, Batch = 200, Batch loss = 0.000029, Avg loss (per batch) = 0.050148\n","05/13/2022 15:57:24 - INFO - __main__ -   Epoch = 6, Batch = 250, Batch loss = 0.000323, Avg loss (per batch) = 0.050215\n","05/13/2022 15:58:37 - INFO - __main__ -   Epoch = 6, Batch = 300, Batch loss = 0.000035, Avg loss (per batch) = 0.045637\n","05/13/2022 15:59:50 - INFO - __main__ -   Epoch = 6, Batch = 350, Batch loss = 0.874690, Avg loss (per batch) = 0.048628\n","05/13/2022 16:01:04 - INFO - __main__ -   Epoch = 6, Batch = 400, Batch loss = 0.000761, Avg loss (per batch) = 0.049272\n","05/13/2022 16:02:17 - INFO - __main__ -   Epoch = 6, Batch = 450, Batch loss = 0.000055, Avg loss (per batch) = 0.045412\n","05/13/2022 16:03:30 - INFO - __main__ -   Epoch = 6, Batch = 500, Batch loss = 0.000303, Avg loss (per batch) = 0.045031\n","05/13/2022 16:04:43 - INFO - __main__ -   Epoch = 6, Batch = 550, Batch loss = 0.000046, Avg loss (per batch) = 0.045762\n","05/13/2022 16:05:57 - INFO - __main__ -   Epoch = 6, Batch = 600, Batch loss = 0.000350, Avg loss (per batch) = 0.045873\n","05/13/2022 16:07:10 - INFO - __main__ -   Epoch = 6, Batch = 650, Batch loss = 0.000352, Avg loss (per batch) = 0.044926\n","05/13/2022 16:08:23 - INFO - __main__ -   Epoch = 6, Batch = 700, Batch loss = 0.000373, Avg loss (per batch) = 0.042808\n","05/13/2022 16:09:36 - INFO - __main__ -   Epoch = 6, Batch = 750, Batch loss = 0.278440, Avg loss (per batch) = 0.043227\n","05/13/2022 16:10:49 - INFO - __main__ -   Epoch = 6, Batch = 800, Batch loss = 0.000030, Avg loss (per batch) = 0.042022\n","05/13/2022 16:12:03 - INFO - __main__ -   Epoch = 6, Batch = 850, Batch loss = 0.000346, Avg loss (per batch) = 0.041749\n","05/13/2022 16:13:16 - INFO - __main__ -   Epoch = 6, Batch = 900, Batch loss = 0.000046, Avg loss (per batch) = 0.040380\n","05/13/2022 16:14:30 - INFO - __main__ -   Epoch = 6, Batch = 950, Batch loss = 0.000244, Avg loss (per batch) = 0.038342\n","05/13/2022 16:15:43 - INFO - __main__ -   Epoch = 6, Batch = 1000, Batch loss = 0.000099, Avg loss (per batch) = 0.041688\n","05/13/2022 16:16:57 - INFO - __main__ -   Epoch = 6, Batch = 1050, Batch loss = 0.000534, Avg loss (per batch) = 0.044783\n","05/13/2022 16:18:11 - INFO - __main__ -   Epoch = 6, Batch = 1100, Batch loss = 0.000024, Avg loss (per batch) = 0.045642\n","05/13/2022 16:18:34 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 16:26:48 - INFO - __main__ -   After 6.000000 epoch, Training loss = 0.045020, Training accuracy = 0.990250\n","05/13/2022 16:28:02 - INFO - __main__ -   Epoch = 7, Batch = 50, Batch loss = 0.000343, Avg loss (per batch) = 0.079072\n","05/13/2022 16:29:16 - INFO - __main__ -   Epoch = 7, Batch = 100, Batch loss = 0.000033, Avg loss (per batch) = 0.043576\n","05/13/2022 16:30:30 - INFO - __main__ -   Epoch = 7, Batch = 150, Batch loss = 0.873324, Avg loss (per batch) = 0.065523\n","05/13/2022 16:31:43 - INFO - __main__ -   Epoch = 7, Batch = 200, Batch loss = 0.000020, Avg loss (per batch) = 0.049240\n","05/13/2022 16:32:57 - INFO - __main__ -   Epoch = 7, Batch = 250, Batch loss = 0.000107, Avg loss (per batch) = 0.043914\n","05/13/2022 16:34:10 - INFO - __main__ -   Epoch = 7, Batch = 300, Batch loss = 0.000066, Avg loss (per batch) = 0.044234\n","05/13/2022 16:35:24 - INFO - __main__ -   Epoch = 7, Batch = 350, Batch loss = 0.000397, Avg loss (per batch) = 0.043332\n","05/13/2022 16:36:37 - INFO - __main__ -   Epoch = 7, Batch = 400, Batch loss = 0.001332, Avg loss (per batch) = 0.039104\n","05/13/2022 16:37:51 - INFO - __main__ -   Epoch = 7, Batch = 450, Batch loss = 0.000037, Avg loss (per batch) = 0.036970\n","05/13/2022 16:39:05 - INFO - __main__ -   Epoch = 7, Batch = 500, Batch loss = 0.000157, Avg loss (per batch) = 0.033428\n","05/13/2022 16:40:18 - INFO - __main__ -   Epoch = 7, Batch = 550, Batch loss = 0.000035, Avg loss (per batch) = 0.033047\n","05/13/2022 16:41:32 - INFO - __main__ -   Epoch = 7, Batch = 600, Batch loss = 0.000124, Avg loss (per batch) = 0.033954\n","05/13/2022 16:42:45 - INFO - __main__ -   Epoch = 7, Batch = 650, Batch loss = 0.000122, Avg loss (per batch) = 0.033440\n","05/13/2022 16:43:58 - INFO - __main__ -   Epoch = 7, Batch = 700, Batch loss = 0.000162, Avg loss (per batch) = 0.031069\n","05/13/2022 16:45:11 - INFO - __main__ -   Epoch = 7, Batch = 750, Batch loss = 0.000138, Avg loss (per batch) = 0.030112\n","05/13/2022 16:46:25 - INFO - __main__ -   Epoch = 7, Batch = 800, Batch loss = 0.000022, Avg loss (per batch) = 0.032999\n","05/13/2022 16:47:38 - INFO - __main__ -   Epoch = 7, Batch = 850, Batch loss = 0.000618, Avg loss (per batch) = 0.033036\n","05/13/2022 16:48:51 - INFO - __main__ -   Epoch = 7, Batch = 900, Batch loss = 0.000030, Avg loss (per batch) = 0.031410\n","05/13/2022 16:50:04 - INFO - __main__ -   Epoch = 7, Batch = 950, Batch loss = 0.000120, Avg loss (per batch) = 0.032083\n","05/13/2022 16:51:17 - INFO - __main__ -   Epoch = 7, Batch = 1000, Batch loss = 0.000096, Avg loss (per batch) = 0.033942\n","05/13/2022 16:52:30 - INFO - __main__ -   Epoch = 7, Batch = 1050, Batch loss = 0.000260, Avg loss (per batch) = 0.035499\n","05/13/2022 16:53:44 - INFO - __main__ -   Epoch = 7, Batch = 1100, Batch loss = 0.000025, Avg loss (per batch) = 0.035959\n","05/13/2022 16:54:06 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 17:02:12 - INFO - __main__ -   After 7.000000 epoch, Training loss = 0.035453, Training accuracy = 0.992267\n","05/13/2022 17:03:25 - INFO - __main__ -   Epoch = 8, Batch = 50, Batch loss = 0.000170, Avg loss (per batch) = 0.003180\n","05/13/2022 17:04:39 - INFO - __main__ -   Epoch = 8, Batch = 100, Batch loss = 0.000022, Avg loss (per batch) = 0.010243\n","05/13/2022 17:05:52 - INFO - __main__ -   Epoch = 8, Batch = 150, Batch loss = 0.000133, Avg loss (per batch) = 0.031249\n","05/13/2022 17:07:05 - INFO - __main__ -   Epoch = 8, Batch = 200, Batch loss = 0.000014, Avg loss (per batch) = 0.023531\n","05/13/2022 17:08:18 - INFO - __main__ -   Epoch = 8, Batch = 250, Batch loss = 0.000062, Avg loss (per batch) = 0.025703\n","05/13/2022 17:09:32 - INFO - __main__ -   Epoch = 8, Batch = 300, Batch loss = 0.000017, Avg loss (per batch) = 0.025885\n","05/13/2022 17:10:45 - INFO - __main__ -   Epoch = 8, Batch = 350, Batch loss = 0.000318, Avg loss (per batch) = 0.028278\n","05/13/2022 17:11:58 - INFO - __main__ -   Epoch = 8, Batch = 400, Batch loss = 0.000350, Avg loss (per batch) = 0.026378\n","05/13/2022 17:13:11 - INFO - __main__ -   Epoch = 8, Batch = 450, Batch loss = 0.000054, Avg loss (per batch) = 0.025156\n","05/13/2022 17:14:24 - INFO - __main__ -   Epoch = 8, Batch = 500, Batch loss = 0.007201, Avg loss (per batch) = 0.022672\n","05/13/2022 17:15:38 - INFO - __main__ -   Epoch = 8, Batch = 550, Batch loss = 0.000019, Avg loss (per batch) = 0.022041\n","05/13/2022 17:16:51 - INFO - __main__ -   Epoch = 8, Batch = 600, Batch loss = 0.399657, Avg loss (per batch) = 0.022140\n","05/13/2022 17:18:04 - INFO - __main__ -   Epoch = 8, Batch = 650, Batch loss = 0.000078, Avg loss (per batch) = 0.020566\n","05/13/2022 17:19:17 - INFO - __main__ -   Epoch = 8, Batch = 700, Batch loss = 0.000178, Avg loss (per batch) = 0.019111\n","05/13/2022 17:20:30 - INFO - __main__ -   Epoch = 8, Batch = 750, Batch loss = 0.000134, Avg loss (per batch) = 0.018773\n","05/13/2022 17:21:43 - INFO - __main__ -   Epoch = 8, Batch = 800, Batch loss = 0.000017, Avg loss (per batch) = 0.018717\n","05/13/2022 17:22:57 - INFO - __main__ -   Epoch = 8, Batch = 850, Batch loss = 0.000084, Avg loss (per batch) = 0.017893\n","05/13/2022 17:24:10 - INFO - __main__ -   Epoch = 8, Batch = 900, Batch loss = 0.000020, Avg loss (per batch) = 0.017205\n","05/13/2022 17:25:23 - INFO - __main__ -   Epoch = 8, Batch = 950, Batch loss = 0.000099, Avg loss (per batch) = 0.016447\n","05/13/2022 17:26:36 - INFO - __main__ -   Epoch = 8, Batch = 1000, Batch loss = 0.000091, Avg loss (per batch) = 0.019821\n","05/13/2022 17:27:49 - INFO - __main__ -   Epoch = 8, Batch = 1050, Batch loss = 0.000133, Avg loss (per batch) = 0.020208\n","05/13/2022 17:29:02 - INFO - __main__ -   Epoch = 8, Batch = 1100, Batch loss = 0.000021, Avg loss (per batch) = 0.020859\n","05/13/2022 17:29:25 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 17:37:31 - INFO - __main__ -   After 8.000000 epoch, Training loss = 0.020564, Training accuracy = 0.995293\n","05/13/2022 17:38:44 - INFO - __main__ -   Epoch = 9, Batch = 50, Batch loss = 0.000101, Avg loss (per batch) = 0.003334\n","05/13/2022 17:39:57 - INFO - __main__ -   Epoch = 9, Batch = 100, Batch loss = 0.000015, Avg loss (per batch) = 0.007049\n","05/13/2022 17:41:10 - INFO - __main__ -   Epoch = 9, Batch = 150, Batch loss = 0.000062, Avg loss (per batch) = 0.019660\n","05/13/2022 17:42:23 - INFO - __main__ -   Epoch = 9, Batch = 200, Batch loss = 0.000012, Avg loss (per batch) = 0.014812\n","05/13/2022 17:43:37 - INFO - __main__ -   Epoch = 9, Batch = 250, Batch loss = 0.000048, Avg loss (per batch) = 0.011885\n","05/13/2022 17:44:50 - INFO - __main__ -   Epoch = 9, Batch = 300, Batch loss = 0.000014, Avg loss (per batch) = 0.012926\n","05/13/2022 17:46:03 - INFO - __main__ -   Epoch = 9, Batch = 350, Batch loss = 0.000099, Avg loss (per batch) = 0.011120\n","05/13/2022 17:47:16 - INFO - __main__ -   Epoch = 9, Batch = 400, Batch loss = 0.955549, Avg loss (per batch) = 0.012133\n","05/13/2022 17:48:29 - INFO - __main__ -   Epoch = 9, Batch = 450, Batch loss = 0.000024, Avg loss (per batch) = 0.012358\n","05/13/2022 17:49:43 - INFO - __main__ -   Epoch = 9, Batch = 500, Batch loss = 0.000079, Avg loss (per batch) = 0.011130\n","05/13/2022 17:50:56 - INFO - __main__ -   Epoch = 9, Batch = 550, Batch loss = 0.000012, Avg loss (per batch) = 0.010226\n","05/13/2022 17:52:09 - INFO - __main__ -   Epoch = 9, Batch = 600, Batch loss = 0.000031, Avg loss (per batch) = 0.009570\n","05/13/2022 17:53:22 - INFO - __main__ -   Epoch = 9, Batch = 650, Batch loss = 0.000042, Avg loss (per batch) = 0.010626\n","05/13/2022 17:54:35 - INFO - __main__ -   Epoch = 9, Batch = 700, Batch loss = 0.000092, Avg loss (per batch) = 0.009894\n","05/13/2022 17:55:49 - INFO - __main__ -   Epoch = 9, Batch = 750, Batch loss = 0.000073, Avg loss (per batch) = 0.010112\n","05/13/2022 17:57:02 - INFO - __main__ -   Epoch = 9, Batch = 800, Batch loss = 0.000011, Avg loss (per batch) = 0.010753\n","05/13/2022 17:58:15 - INFO - __main__ -   Epoch = 9, Batch = 850, Batch loss = 0.000046, Avg loss (per batch) = 0.010129\n","05/13/2022 17:59:28 - INFO - __main__ -   Epoch = 9, Batch = 900, Batch loss = 0.000014, Avg loss (per batch) = 0.009614\n","05/13/2022 18:00:41 - INFO - __main__ -   Epoch = 9, Batch = 950, Batch loss = 0.000061, Avg loss (per batch) = 0.009111\n","05/13/2022 18:01:55 - INFO - __main__ -   Epoch = 9, Batch = 1000, Batch loss = 0.000190, Avg loss (per batch) = 0.015188\n","05/13/2022 18:03:08 - INFO - __main__ -   Epoch = 9, Batch = 1050, Batch loss = 0.000119, Avg loss (per batch) = 0.020626\n","05/13/2022 18:04:21 - INFO - __main__ -   Epoch = 9, Batch = 1100, Batch loss = 0.000166, Avg loss (per batch) = 0.020324\n","05/13/2022 18:04:44 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 18:12:50 - INFO - __main__ -   After 9.000000 epoch, Training loss = 0.020034, Training accuracy = 0.995853\n","05/13/2022 18:14:03 - INFO - __main__ -   Epoch = 10, Batch = 50, Batch loss = 0.000087, Avg loss (per batch) = 0.023565\n","05/13/2022 18:15:16 - INFO - __main__ -   Epoch = 10, Batch = 100, Batch loss = 0.000016, Avg loss (per batch) = 0.018590\n","05/13/2022 18:16:29 - INFO - __main__ -   Epoch = 10, Batch = 150, Batch loss = 0.000063, Avg loss (per batch) = 0.039007\n","05/13/2022 18:17:43 - INFO - __main__ -   Epoch = 10, Batch = 200, Batch loss = 0.000012, Avg loss (per batch) = 0.035418\n","05/13/2022 18:18:56 - INFO - __main__ -   Epoch = 10, Batch = 250, Batch loss = 0.000033, Avg loss (per batch) = 0.028636\n","05/13/2022 18:20:09 - INFO - __main__ -   Epoch = 10, Batch = 300, Batch loss = 0.000018, Avg loss (per batch) = 0.026652\n","05/13/2022 18:21:22 - INFO - __main__ -   Epoch = 10, Batch = 350, Batch loss = 0.000101, Avg loss (per batch) = 0.022855\n","05/13/2022 18:22:36 - INFO - __main__ -   Epoch = 10, Batch = 400, Batch loss = 0.000150, Avg loss (per batch) = 0.020012\n","05/13/2022 18:23:49 - INFO - __main__ -   Epoch = 10, Batch = 450, Batch loss = 0.000032, Avg loss (per batch) = 0.017879\n","05/13/2022 18:25:02 - INFO - __main__ -   Epoch = 10, Batch = 500, Batch loss = 0.000062, Avg loss (per batch) = 0.016097\n","05/13/2022 18:26:15 - INFO - __main__ -   Epoch = 10, Batch = 550, Batch loss = 0.000011, Avg loss (per batch) = 0.014989\n","05/13/2022 18:27:28 - INFO - __main__ -   Epoch = 10, Batch = 600, Batch loss = 0.000040, Avg loss (per batch) = 0.014069\n","05/13/2022 18:28:42 - INFO - __main__ -   Epoch = 10, Batch = 650, Batch loss = 0.000029, Avg loss (per batch) = 0.012991\n","05/13/2022 18:29:55 - INFO - __main__ -   Epoch = 10, Batch = 700, Batch loss = 0.000070, Avg loss (per batch) = 0.012098\n","05/13/2022 18:31:08 - INFO - __main__ -   Epoch = 10, Batch = 750, Batch loss = 0.000055, Avg loss (per batch) = 0.011970\n","05/13/2022 18:32:21 - INFO - __main__ -   Epoch = 10, Batch = 800, Batch loss = 0.000012, Avg loss (per batch) = 0.011235\n","05/13/2022 18:33:35 - INFO - __main__ -   Epoch = 10, Batch = 850, Batch loss = 0.000044, Avg loss (per batch) = 0.010577\n","05/13/2022 18:34:48 - INFO - __main__ -   Epoch = 10, Batch = 900, Batch loss = 0.000019, Avg loss (per batch) = 0.009994\n","05/13/2022 18:36:01 - INFO - __main__ -   Epoch = 10, Batch = 950, Batch loss = 0.000048, Avg loss (per batch) = 0.010110\n","05/13/2022 18:37:14 - INFO - __main__ -   Epoch = 10, Batch = 1000, Batch loss = 0.008228, Avg loss (per batch) = 0.009713\n","05/13/2022 18:38:27 - INFO - __main__ -   Epoch = 10, Batch = 1050, Batch loss = 0.000263, Avg loss (per batch) = 0.015451\n","05/13/2022 18:39:41 - INFO - __main__ -   Epoch = 10, Batch = 1100, Batch loss = 0.000012, Avg loss (per batch) = 0.025721\n","05/13/2022 18:40:03 - INFO - __main__ -   Creating a checkpoint.\n","05/13/2022 18:48:09 - INFO - __main__ -   After 10.000000 epoch, Training loss = 0.026413, Training accuracy = 0.995069\n"]}],"source":["# train set : \"data/sentihood/bert-sentclass-processed/processed_train_QA_M.tsv\"\n","!python3 bert_sentclass.py --num_train_epochs 10 --max_seq_length 512 --batch_size 8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1239,"status":"ok","timestamp":1652468050841,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"LIcHSCp0KkW5","outputId":"0f5da67c-b073-41f0-853d-e250b2f8a383"},"outputs":[{"name":"stdout","output_type":"stream","text":["aspect_strict_Acc = 0.3139913232104121\n","aspect_Macro_F1 = 0.5285950401238133\n","aspect_Macro_AUC = 0.8604129494905468\n","sentiment_Acc = 0.9021739130434783\n","sentiment_Macro_AUC = 0.9503315485547762\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_6.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1231,"status":"ok","timestamp":1652468074530,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"Yib74npgiT01","outputId":"607d6b70-524b-4502-e407-40f5174eba47"},"outputs":[{"name":"stdout","output_type":"stream","text":["aspect_strict_Acc = 0.24837310195227766\n","aspect_Macro_F1 = 0.5031891521052381\n","aspect_Macro_AUC = 0.814109955962097\n","sentiment_Acc = 0.9188963210702341\n","sentiment_Macro_AUC = 0.9465980849902023\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_7.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1185,"status":"ok","timestamp":1652468081450,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"pg-yJWk8nWpj","outputId":"5ab7d5d5-4c70-4af8-f293-e95426a8185c"},"outputs":[{"name":"stdout","output_type":"stream","text":["aspect_strict_Acc = 0.2678958785249458\n","aspect_Macro_F1 = 0.5007955593655886\n","aspect_Macro_AUC = 0.8245660364641509\n","sentiment_Acc = 0.9205685618729097\n","sentiment_Macro_AUC = 0.9503861906353999\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_8.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1233,"status":"ok","timestamp":1652468085864,"user":{"displayName":"Firas CHERIF","userId":"05000170014321048478"},"user_tz":240},"id":"33uR2RG2Q7S4","outputId":"19269d71-d502-4960-f6f6-5d4cf5b9b9c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["aspect_strict_Acc = 0.3183297180043384\n","aspect_Macro_F1 = 0.5371498979377223\n","aspect_Macro_AUC = 0.838535396317701\n","sentiment_Acc = 0.9188963210702341\n","sentiment_Macro_AUC = 0.954287109011351\n"]}],"source":["!python evaluation_sentclass.py --pred_data_dir results/bert_sentclass_ep_9.txt"]},{"cell_type":"markdown","metadata":{"id":"iBmAuJc45Npb"},"source":["Supprimer les \"none\" du train set lorsqu'au moins un aspect est présent : \\\n","Les résultats ont chuté surtout pour la détection d'aspect.\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"launcher_notebook_bert_processed.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
